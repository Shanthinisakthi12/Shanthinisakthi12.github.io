---
layout: post
title: "Build a Context-Aware AI Meeting Assistant with MCP and Hugging Face-A tutorial"
date: 2025-10-10
categories: [MCP]
---

## Introduction

Meetings are a vital part of any organization, but capturing the key points, action items, and decisions can be tedious. What if you had an AI assistant that could **listen, summarize, and generate structured action tables automatically**?  

This is where **Model-Context-Protocol (MCP)** comes into play. MCP is a framework that allows multiple AI models to work together in a **structured, context-aware pipeline**, passing information seamlessly from one model to another. In this tutorial, we’ll build a **context-aware meeting assistant** using Hugging Face models that transcribes audio, summarizes discussions, and outputs a **ready-to-use table** with action items.  

---

## What is MCP?

**Model-Context-Protocol (MCP)** is an approach to orchestrate AI models:

- **Model:** The AI components (e.g., transcription model, summarization model).  
- **Context:** The information passed between models (e.g., meeting transcript).  
- **Protocol:** The structured rules for interaction (e.g., output format, table structure).  

In this project, MCP works as follows:

1. **Transcription Model:** Converts meeting audio into text.  
2. **Summarization Model:** Consumes the transcript (context) and outputs structured tables.  
3. **Protocol:** Ensures the summarizer outputs **Markdown tables** with specific columns: *Topic, Discussion Summary, Decision, Action Item, Owner, Deadline, Notes*.  

*(Insert diagram here showing MCP flow: audio → transcription model → summarizer → structured table. Reference: [Anthropic MCP overview](https://www.anthropic.com/))*

---

## Step-by-Step Tutorial

### Step 1: Install Dependencies

We’ll use Hugging Face Transformers for both transcription and summarization, along with `gspread` to save tables to Google Sheets.

<code>
!pip install transformers torchaudio soundfile gspread gspread_dataframe pyarrow requests
</code>
